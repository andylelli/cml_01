# LLM Generation Map

**Date**: February 9, 2026  
**Purpose**: Document exactly which parts of the system are LLM-generated and where in the workflow each generation occurs

---

## Overview

The CML mystery generation system uses LLMs at specific stages to create content. This document maps:
- **What** is generated by LLMs
- **When** in the workflow generation occurs
- **Which** agent/prompt handles each generation
- **What** the inputs and outputs are for each step
- **Current status** of implementation (Phase 1 vs planned)

---

## Quick Reference: LLM vs Non-LLM

### üéØ Designed For LLM (Currently Deterministic in Phase 1)
All of these are **planned to be LLM-generated** but are currently implemented as deterministic placeholders:

1. **Setting Bible** - Era constraints and realism rules
   - *Current*: Deterministic derivation from spec (decade, location, static weather/social structure)
   - *Planned*: LLM Agent 1 generates era-appropriate constraints and realism rules

2. **Cast Details** - Character secrets, motives, alibis, relationships
   - *Current*: Uses spec names or defaults (Avery, Blair, Casey, etc.)
   - *Planned*: LLM Agent 2 generates rich character backgrounds and relationships

3. **CML Document** - Complete mystery case logic (core artifact)
   - *Current*: Hardcoded temporal axis example with poisoning
   - *Planned*: LLM Agent 3 generates full CML 2.0 based on spec and seeds

4. **CML Revisions** - Fixes to validation failures
   - *Current*: Manual fixes only
   - *Planned*: LLM Agent 4 auto-corrects validation errors

5. **Clue List** - Fair-play clues and red herrings derived from CML
   - *Current*: Template-based with variation by axis/decade/location/cast
   - *Planned*: LLM Agent 5 derives clues from CML facts

6. **Outline** - Chapter structure with clue placement
   - *Current*: Deterministic structure based on spec tone
   - *Planned*: LLM Agent 6 generates narrative arc with strategic clue placement

7. **Prose** - Narrative text for each chapter
   - *Current*: Placeholder paragraphs (2-3 generic sentences per chapter)
   - *Planned*: LLM Agent 7 writes full narrative with style matching

8. **Game Pack** - Interactive mystery game materials
   - *Current*: Basic template with suspect names
   - *Planned*: LLM Agent 8 creates detailed cards and host materials

### ‚ùå Never LLM-Generated (Deterministic/User Input)
These are permanently non-LLM operations:

- Project metadata (name, ID, timestamps)
- User specifications (decade, location, tone, cast size, cast names, primary axis)
- Schema validation (rule-based checking against CML 2.0 schema)
- Novelty audit scoring (similarity calculation algorithm)
- Synopsis extraction (derived from CML metadata fields)
- Fair-play report (checklist evaluation from CML + clues)
- Export packaging (file bundling and PDF generation)
- UI state management (tabs, navigation, form state)

---

## Pipeline Stage-by-Stage Breakdown

### Stage 1: Setting Generation
**Workflow Position**: First stage after "Run Pipeline" is clicked

**What Is Generated**:
- Setting bible with era constraints
- Technology availability (phones, cars, trains, etc.)
- Policing methods (forensics, communication, jurisdiction)
- Social structure and norms
- Travel and communication limitations
- Realism constraints for the chosen decade

**LLM Agent**: Agent 1 ‚Äî Era & Setting

**Inputs**:
- User spec: `decade` (1930s/1940s/1950s)
- User spec: `locationPreset` (CountryHouse/SeasideHotel/Village/Liner/Theatre)
- User spec: `tone` (Cozy/Classic/Dark)
- Optional: User notes or preferences

**Outputs**:
- Artifact type: `setting`
- Structure: `{ decade, locationPreset, weather, socialStructure, institution }`
- Validation: Checked for anachronisms and consistency

**Current Status**: ‚úÖ Implemented (deterministic derivation in Phase 1; LLM integration planned)

**Events Emitted**:
- `setting_done` - "Setting generated"

---

### Stage 2: Cast Generation
**Workflow Position**: Second stage, after setting is validated

**What Is Generated**:
- Complete cast list with character names
- Character secrets (hidden information)
- Character motives (reasons for actions)
- Character alibis (time and location claims)
- Character access levels (who could do what)
- Relationship web between characters

**LLM Agent**: Agent 2 ‚Äî Cast & Motive

**Inputs**:
- User spec: `castSize` (4-12 characters)
- User spec: `castNames` (optional comma-separated names)
- Setting bible from Stage 1
- User spec: `tone` (affects character complexity)

**Outputs**:
- Artifact type: `cast`
- Structure: `{ size, detectiveType, victimArchetype, suspects[] }`
- Each suspect: Name, role placeholder, eventual access notes
- Validation: Required fields, no harmful stereotypes

**Current Status**: ‚úÖ Implemented (deterministic in Phase 1; names from spec or defaults)

**Events Emitted**:
- `cast_done` - "Cast generated"

---

### Stage 3: CML Generation (CORE)
**Workflow Position**: Third stage, the central artifact that drives everything downstream

**What Is Generated**:
- **Complete CML 2.0 document** (the canonical mystery logic)
- Case metadata (title, author, era, setting)
- Cast with detailed attributes and culpability status
- **Culpability** (who is guilty, culprit count)
- **Surface model** (what the reader sees):
  - Narrative framing
  - Accepted facts
  - Inferred conclusions
- **Hidden model** (the truth):
  - Mechanism (how the crime was done)
  - Delivery path (steps taken)
  - Outcome (what happened)
- **False assumption** (the misdirection):
  - Type (temporal/spatial/identity/behavioral/authority)
  - Reasoning (why it's believed)
  - What it hides
- **Constraint space** (boundaries):
  - Time constraints
  - Access constraints
  - Physical constraints
  - Social constraints
- **Inference path** (how to solve it):
  - Observation sequence
  - Logic chain
  - Conclusion steps
- **Discriminating test** (the proof):
  - Method (reenactment/trap/constraint_proof/administrative_pressure)
  - Execution steps
  - Expected outcome
- **Fair play checklist** items

**LLM Agent**: Agent 3 ‚Äî Crime Designer (CML Generator)

**Inputs**:
- Setting bible from Stage 1
- Cast from Stage 2
- User spec: `primaryAxis` (temporal/spatial/identity/behavioral/authority)
- Logic knobs (mechanism families, complexity preferences)
- **Structural inspiration only** from seed CMLs (never content copying)

**Outputs**:
- Artifact type: `cml`
- Full CML 2.0 YAML document
- Validation: Schema compliance + checklist validation
- Novelty: Must not be too similar to any single seed CML

**Current Status**: ‚úÖ Implemented (deterministic "temporal" axis example in Phase 1; LLM integration planned)

**Events Emitted**:
- `cml_retry` - "CML failed validation; retrying" (if validation fails)
- `cml_validated` - "CML validated" (after passing)

**Special Notes**:
- **CML is ALWAYS generated** - Every mystery has a CML
- **CML is hidden by default** - Only visible in Advanced/Expert modes
- **Novelty enforcement** - If CML is too similar to a seed, regenerate with divergence constraints
- **No seed copying** - Seeds provide abstract structure only (axis patterns, mechanism families), never specific characters, events, clue wording, or logic chains

---

### Stage 3.5: CML Revision (If Needed)
**Workflow Position**: Triggered automatically if Stage 3 validation fails

**What Is Generated**:
- **Corrected CML 2.0 document** with specific fixes
- Targeted repairs to validation failures
- Must not introduce new facts outside constraint space

**LLM Agent**: Agent 4 ‚Äî CML Validator (revision mode)

**Inputs**:
- Failed validation report (specific errors)
- Current CML document
- Constraint space from original generation
- Novelty divergence requirements (if similarity triggered revision)

**Outputs**:
- Artifact type: `cml` (updated)
- Fixed CML document
- Re-validation: Schema + checklist + novelty audit

**Current Status**: üîÑ Planned (manual fixes in Phase 1; automated LLM revision in later phase)

**Events Emitted**:
- `cml_retry` - "CML failed validation; retrying"
- `cml_validated` - "CML validated" (after successful fix)

---

### Stage 4: Synopsis Generation
**Workflow Position**: After CML is validated

**What Is Generated**:
- Brief case summary (1-2 sentences)
- Title extraction
- Status indicator

**LLM Agent**: ‚ùå None (deterministic extraction)

**Inputs**:
- Validated CML metadata section

**Outputs**:
- Artifact type: `synopsis`
- Structure: `{ status, title, summary }`

**Current Status**: ‚úÖ Implemented (extracts from CML metadata)

**Events Emitted**:
- `synopsis_done` - "Synopsis generated"

**Notes**: Not LLM-generated; simply extracts `title` and creates summary from CML metadata

---

### Stage 5: Novelty Audit
**Workflow Position**: After CML generation, validates uniqueness

**What Is Generated**:
- Novelty score comparing CML to seed examples
- Similarity patterns identified
- Pass/fail decision

**LLM Agent**: ‚ùå None (deterministic similarity calculation)

**Inputs**:
- Generated CML document
- Seed CMLs from `examples/` directory
- Similarity threshold configuration

**Outputs**:
- Artifact type: `novelty_audit`
- Structure: `{ status: "pass"|"fail", seedIds: [], patterns: [], score: number }`

**Current Status**: ‚ö†Ô∏è Stub (always passes in Phase 1; real similarity engine planned)

**Events Emitted**:
- `novelty_audit` - "Novelty audit passed" or "Novelty audit failed; regeneration required"

**Notes**: 
- Not LLM-generated; uses deterministic similarity algorithms
- If fails: triggers CML regeneration with explicit divergence constraints
- Ensures no single seed is copied

---

### Stage 6: Clues Generation
**Workflow Position**: After CML is validated and novel

**What Is Generated**:
- **Fair-play clue list** organized by category
- Genuine clues pointing to the culprit
- **Red herrings** supporting the false assumption
- Clue timing (which chapter reveals each clue)
- Clue density balancing

**LLM Agent**: Agent 5 ‚Äî Clue & Red Herrings

**Inputs**:
- Validated CML document (surface + hidden models)
- False assumption from CML
- User spec: `clueDensity` (light/medium/heavy)
- User spec: `redHerringBudget` (how many misdirections)

**Outputs**:
- Artifact type: `clues`
- Structure: `{ status, density, axis, summary, items[] }`
- Each clue: `{ id, category, text, pointsTo, redHerring: boolean, revealChapter }`
- Categories: STATEMENT, PHYSICAL, DOCUMENT, TESTIMONY, BEHAVIOR, CIRCUMSTANTIAL, TECHNICAL
- Validation: All clues must be grounded in CML facts; no new facts added

**Current Status**: ‚úÖ Implemented (deterministic derivation from CML; LLM enhancement planned)

**Events Emitted**:
- `clues_done` - "Clues generated"

**Special Notes**:
- **No new facts rule**: Clues can only reveal what's already in CML
- **Red herrings**: Must support the false assumption without breaking fairness
- **Fair play**: Load-bearing clues must appear before solution

---

### Stage 7: Fair Play Report
**Workflow Position**: After clues are generated

**What Is Generated**:
- Fair-play validation checks
- Solvability assessment

**LLM Agent**: ‚ùå None (deterministic checklist evaluation)

**Inputs**:
- Validated CML document
- Generated clues list

**Outputs**:
- Artifact type: `fair_play_report`
- Structure: `{ status, summary, checks[] }`
- Checks: all_clues_visible, no_late_information, reader_can_solve, red_herrings_controlled

**Current Status**: ‚úÖ Implemented (deterministic validation)

**Events Emitted**:
- `fair_play_report_done` - "Fair-play report generated"

**Notes**: Not LLM-generated; validates CML and clues against fair-play rules

---

### Stage 8: Outline Generation
**Workflow Position**: After fair-play report passes

**What Is Generated**:
- Chapter-by-chapter story structure
- Chapter titles and summaries
- **Clue placement** (which clues appear in which chapters)
- Act structure (setup, investigation, revelation, resolution)
- Timing of discriminating test
- Pacing and tension curve

**LLM Agent**: Agent 6 ‚Äî Narrative Outliner

**Inputs**:
- Validated CML document
- Clues list with timing requirements
- Cast information
- User spec: `tone` (affects pacing)
- Setting constraints

**Outputs**:
- Artifact type: `outline`
- Structure: `{ status, tone, chapters[], summary }`
- Each chapter: `{ title, summary, events[], cluesRevealed[] }`
- Validation: Load-bearing clues appear before solution; discriminating test placed late

**Current Status**: ‚úÖ Implemented (deterministic structure; LLM enhancement planned)

**Events Emitted**:
- `outline_done` - "Outline generated"

**Special Notes**:
- **Clue placement critical**: Must follow fair-play ordering
- **No logic changes**: Outline cannot alter CML facts
- **Discriminating test timing**: Must appear late enough to be satisfying

---

### Stage 9: Prose Generation (Optional)
**Workflow Position**: After outline is validated

**What Is Generated**:
- **Narrative prose** for each chapter
- Character dialogue and descriptions
- Scene setting and atmosphere
- Clue presentations embedded in narrative
- Style-matched writing (cozy/classic/dark tone)

**LLM Agent**: Agent 7 ‚Äî Prose Writer

**Inputs**:
- Validated outline with chapter structure
- Cast with character details
- Setting bible for atmosphere
- User spec: `style` (writing style capture or descriptor)
- User spec: `tone` (cozy/classic/dark)
- User spec: `pov` (first-person detective, third-person omniscient, etc.)

**Outputs**:
- Artifact type: `prose`
- Structure: `{ status, tone, chapters[], cast[], note }`
- Each chapter: `{ title, summary, paragraphs[] }`
- Validation: Must not introduce new facts; must match style without copying copyrighted text

**Current Status**: ‚ö†Ô∏è **Planned (Phase 2+)** - Currently generates deterministic placeholder prose

**Current Placeholder**:
- Generates 3-5 generic paragraphs per chapter
- Uses chapter summary and cast names
- No LLM involved in Phase 1

**Future LLM Implementation**:
- Full narrative generation with style matching
- Character voice consistency
- Atmospheric scene-setting
- Natural clue integration

**Events Emitted**:
- `prose_done` - "Prose generated"

**Special Notes**:
- **No new facts rule**: Prose cannot alter CML logic
- **Prose is downstream**: Always generated from validated CML
- **Style capture**: Uses user-provided sample or descriptor to match tone/voice
- **Copyright safety**: Must not replicate copyrighted text

---

### Stage 10: Game Pack Generation (Optional)
**Workflow Position**: After all core artifacts are complete

**What Is Generated**:
- **Suspect cards** (one per cast member with role, secrets, alibis)
- **Host packet** (instructions for running the mystery game)
- **Timeline sheet** (event chronology)
- **Handouts** (clue cards, documents, letters)
- **Solution reveal** (for game master)
- **Printable materials** (formatted for physical game)

**LLM Agent**: Agent 8 ‚Äî Game Pack (optional)

**Inputs**:
- Validated CML document
- Cast with full character details
- Clues list
- Outline for timeline reference

**Outputs**:
- Artifact type: `game_pack`
- Structure: `{ title, suspects[], hostPacket, materials[], timeline }`
- Validation: All facts consistent with CML

**Current Status**: ‚ö†Ô∏è **Planned (Phase 2+)** - Currently generates deterministic placeholder

**Current Placeholder**:
- Generates basic suspect cards with names
- Simple host packet template
- Generic materials list
- No LLM involved in Phase 1

**Future LLM Implementation**:
- Detailed character cards with secrets and motives
- Customized host instructions
- Interactive game elements
- Formatted printable PDFs

**Events Emitted**:
- `game_pack_done` - "Game pack generated"

**Special Notes**:
- **Consistency critical**: All facts must match CML
- **Spoiler management**: Different information for players vs host
- **Printable format**: Designed for physical mystery party use

---

## Regeneration Workflow

### User-Triggered Regeneration
**When**: User clicks "Regenerate" for a specific artifact in the UI

**Process**:
1. User selects scope: `setting`, `cast`, `cml`, `clues`, `outline`, `prose`, or `game_pack`
2. System fetches latest spec for the project
3. System re-runs **only the selected LLM agent** for that artifact
4. Dependencies are handled:
   - Regenerating `prose` requires `outline` and `cast`
   - Regenerating `game_pack` requires `cml` and `cast`
   - Regenerating `outline` requires `clues` and `cml`
   - Regenerating `clues` requires `cml`

**LLM Usage**: Same as initial generation for that artifact type

**Current Status**: ‚úÖ Implemented (endpoint exists; re-runs derivation logic)

**Events Emitted**:
- `${scope}_regenerated` - "Setting regenerated" / "CML regenerated" / etc.

---

## Access Control & CML Visibility

### Information Access Levels

**Level 1 ‚Äî User (Default)**
- **No raw CML access** (403 Forbidden on CML endpoints)
- See: Friendly projections (cast list, clue board, outline, prose)
- Cannot see: CML document, validation details, schema structure

**Level 2 ‚Äî Advanced (Opt-in)**
- **Read-only CML access** via CML Viewer tab
- See: Complete CML YAML, validation results, schema structure
- Cannot: Edit CML, validate custom CML

**Level 3 ‚Äî Expert (Explicit Opt-in)**
- **Full CML access** including editing and validation
- See: Everything in Advanced mode
- Can: Edit CML directly, validate changes, control regeneration scope

### CML-Protected Endpoints
All require `x-cml-mode: advanced` or `x-cml-mode: expert` header:
- `GET /api/projects/:id/cml/latest` - Fetch CML document
- `POST /api/projects/:id/cml/validate` - Validate CML payload
- `GET /api/projects/:id/cml/validation/latest` - Fetch validation results

---

## Sample CML Usage

### What Are Seed CMLs?
- Located in `examples/` directory
- Classic mystery cases in CML 2.0 format:
  - A Study in Scarlet
  - The Moonstone
  - The Mysterious Affair at Styles
  - The Leavenworth Case
  - The Mystery of the Yellow Room
  - The Sign of the Four
  - The Valley of Fear
  - And more...

### How Samples Are Used

**‚úÖ Allowed Uses (Structural Inspiration)**:
- Analyze abstract structure patterns (axis types, mechanism families)
- Learn classic mystery pacing and cadence
- Understand fair-play clue placement strategies
- Study discriminating test methods
- Quality baseline for validation

**‚ùå Forbidden Uses (Content Copying)**:
- Copying specific characters, names, or personalities
- Copying event sequences or plot points
- Copying clue wording or inference chains
- Copying reveal logic or solution methods
- Using seed as a template for generation

### Novelty Enforcement
- **Similarity audit**: Compares generated CML to all seed CMLs
- **Threshold**: Must not be too similar to any single seed
- **If failed**: CML regeneration triggered with explicit divergence constraints
- **Goal**: Ensure every generated mystery is novel and not traceable to any seed

**LLM Agent for Sample Analysis**: Optional (Agent 9 ‚Äî Sample Analyzer)
- **Current**: Samples loaded at startup, used as-is
- **Future**: May add LLM-based summarization or normalization
- **Never**: LLM should not use samples as generation templates

---

## Summary Tables

### LLM Agents & Their Outputs

| Agent | Name | Output Artifact | Input Sources | Status |
|-------|------|----------------|---------------|--------|
| 1 | Era & Setting | `setting` | User spec (decade, location, tone) | ‚úÖ Planned |
| 2 | Cast & Motive | `cast` | Setting + user spec (cast size, names) | ‚úÖ Planned |
| 3 | Crime Designer | `cml` | Setting + cast + logic spec + seed structure | ‚úÖ Planned |
| 4 | CML Validator | `cml` (revised) | Failed validation + current CML | üîÑ Planned |
| 5 | Clue & Red Herrings | `clues` | CML + clue density spec | ‚úÖ Planned |
| 6 | Narrative Outliner | `outline` | CML + clues + cast + tone | ‚úÖ Planned |
| 7 | Prose Writer | `prose` | Outline + cast + style spec | ‚ö†Ô∏è Phase 2+ |
| 8 | Game Pack | `game_pack` | CML + cast + clues | ‚ö†Ô∏è Phase 2+ |
| 9 | Sample Analyzer | Sample summaries | Seed CMLs from examples/ | üîÑ Optional |

### Pipeline Stage LLM Status

| Stage | Artifact | LLM Required? | Current Phase 1 | Future Phases |
|-------|----------|---------------|-----------------|---------------|
| 1 | Setting | ‚úÖ Yes | Deterministic | LLM-enhanced |
| 2 | Cast | ‚úÖ Yes | Deterministic | LLM-generated |
| 3 | CML | ‚úÖ Yes | Deterministic example | LLM-generated |
| 3.5 | CML Revision | ‚úÖ Yes | Manual | LLM auto-fix |
| 4 | Synopsis | ‚ùå No | Extraction | Extraction |
| 5 | Novelty Audit | ‚ùå No | Stub | Similarity engine |
| 6 | Clues | ‚úÖ Yes | Deterministic | LLM-derived |
| 7 | Fair Play | ‚ùå No | Checklist | Checklist |
| 8 | Outline | ‚úÖ Yes | Deterministic | LLM-generated |
| 9 | Prose | ‚úÖ Yes | **Placeholder** | **LLM-written** |
| 10 | Game Pack | ‚úÖ Yes | **Placeholder** | **LLM-created** |

### User Input vs LLM Output

| Data Type | Source | LLM Involved? |
|-----------|--------|---------------|
| Project name | User input | ‚ùå No |
| Decade | User input | ‚ùå No |
| Location preset | User input | ‚ùå No |
| Tone | User input | ‚ùå No |
| Cast size | User input | ‚ùå No |
| Cast names | User input (optional) | ‚ùå No |
| Primary axis | User input | ‚ùå No |
| Setting bible | **LLM generated** | ‚úÖ Yes |
| Cast details | **LLM generated** | ‚úÖ Yes |
| CML document | **LLM generated** | ‚úÖ Yes |
| Clue list | **LLM generated** | ‚úÖ Yes |
| Outline | **LLM generated** | ‚úÖ Yes |
| Prose | **LLM generated** (future) | ‚úÖ Yes |
| Game pack | **LLM generated** (future) | ‚úÖ Yes |
| Validation results | Deterministic | ‚ùå No |
| Fair-play report | Deterministic | ‚ùå No |
| Synopsis | Extraction | ‚ùå No |

---

## Key Principles

### 1. CML is Always Generated
- Every mystery has a CML document at its core
- CML is the source of truth for all downstream artifacts
- CML is hidden by default; visible only in Advanced/Expert modes

### 2. No New Facts Downstream
- Once CML is validated, no downstream agent can add facts
- Clues, outline, prose, game pack must only reveal what's in CML
- This ensures logical consistency throughout

### 3. Novelty is Enforced
- Seed CMLs provide structural inspiration only
- Generated CML must not be too similar to any single seed
- Similarity audit triggers regeneration with divergence constraints

### 4. Fair Play is Validated
- Clues must be grounded in CML facts
- Load-bearing clues must appear before solution
- Discriminating test must be sound and placed correctly
- Red herrings must support false assumption without breaking fairness

### 5. User Control at Every Stage
- Users can accept or reject each artifact
- Users can regenerate individual artifacts
- Expert users can edit CML directly
- All changes are validated before acceptance

---

## Future Enhancements

### Phase 2: Full LLM Integration
- Implement all LLM agents (currently deterministic placeholders)
- Add proper prose generation with style matching
- Add game pack generation with interactive elements
- Add CML auto-revision on validation failure

### Phase 3: Advanced Features
- Sample CML summarization and analysis
- Multi-agent collaboration for complex cases
- Style transfer learning for prose consistency
- Interactive regeneration with user constraints

### Phase 4: Optimization
- Model routing (reasoning models for validation, fast models for outline)
- Caching and reuse of partial results
- Parallel generation where possible
- Cost optimization and token usage tracking

---

## Conclusion

**LLM Touchpoints**: 6-8 stages (depending on optional artifacts)
**Critical LLM Stage**: CML generation (Agent 3) - the core mystery logic
**Current Status**: Phase 1 uses deterministic logic; LLM integration planned
**Access Control**: CML is hidden by default; LLM outputs are projections for users

For questions about specific LLM integration details, see:
- [documentation/04_llm_ai.md](documentation/04_llm_ai.md) - Detailed LLM strategy
- [documentation/02_cml_and_agents.md](documentation/02_cml_and_agents.md) - Agent responsibilities
- [documentation/06_workflow.md](documentation/06_workflow.md) - Pipeline execution flow

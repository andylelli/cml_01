# Phase 2: Full LLM Integration Implementation Plan

**Date**: February 9, 2026  
**Purpose**: Comprehensive plan for implementing all 8 LLM agents with full logging, error handling, and production readiness  
**Estimated Timeline**: 3-4 weeks  
**Status**: Planning

---

## Executive Summary

This plan outlines the complete implementation of Phase 2, transitioning from deterministic placeholders to full LLM-powered mystery generation. The core principle is that **CML is the backbone** - once the LLM generates a valid CML document, all downstream artifacts derive from it.

**Key Deliverables**:
- Azure OpenAI client integration with retry logic and rate limiting
- 8 production-ready LLM agents with structured prompts
- Comprehensive logging system (request/response tracking, telemetry, cost analysis)
- Error handling and fallback strategies
- Testing framework for LLM outputs
- Documentation and monitoring dashboards

---

## Phase 2 Architecture Overview

```
User Spec → [Agent 1: Setting] → Setting Bible
                ↓
         [Agent 2: Cast] → Cast Details
                ↓
         [Agent 3: CML] → **CML Document** (CORE)
                ↓
         Schema Validator → Validation Report
                ↓ (if fail)
         [Agent 4: Revision] → Fixed CML
                ↓
         Novelty Audit → Pass/Fail
                ↓
         [Agent 5: Clues] → Clue List (from CML)
                ↓
         [Agent 6: Outline] → Chapter Structure (from CML + Clues)
                ↓
         [Agent 7: Prose] → Narrative Text (from Outline)
                ↓
         [Agent 8: Game Pack] → Game Materials (from CML)
```

**Critical Flow**:
1. CML is generated by Agent 3 (the backbone)
2. CML is validated and optionally revised by Agent 4
3. All downstream artifacts derive from validated CML
4. No downstream agent can add facts not in CML

---

## Implementation Phases

### Week 1: Infrastructure & Core CML Agent
**Goal**: Set up Azure OpenAI client, logging infrastructure, and implement Agent 3 (CML Generator)

**Day 1-2: Azure OpenAI Setup** ✅ COMPLETED
- [x] Create Azure OpenAI resource (user must configure in Azure Portal)
- [x] Configure API keys and endpoints (.env.local.example created)
- [x] Set up environment variables (.env.local)
- [x] Create shared LLM client package (packages/llm-client/)
- [x] Implement retry logic with exponential backoff (retry.ts)
- [x] Add rate limiting and token tracking (ratelimit.ts, cost-tracker.ts)
- [x] Create logging infrastructure (logger.ts with JSONL output)
- [x] All 13 unit tests passing

**Day 3-4: Agent 3 - CML Generator (CORE)** ✅ COMPLETED
- [x] Design prompt template for CML generation
- [x] Implement YAML output (not JSON mode)
- [x] Add seed CML loading and structure analysis
- [x] Create novelty constraint injection
- [x] Build CML validation loop (generate → validate → retry)
- [x] Add comprehensive logging
- [x] Test with multiple axes and settings
- [x] All 16 unit tests passing

**Day 5: Testing & Validation**
- [ ] Unit tests for CML generation
- [ ] Integration tests for full pipeline
- [ ] Validate against all 5 axis types
- [ ] Stress test with various specs
- [ ] Performance benchmarking

---

### Week 2: Upstream Agents (Setting & Cast) + Revision Agent
**Goal**: Implement pre-CML agents and CML auto-correction

**Day 6-7: Agent 1 - Era & Setting**
- [ ] Design prompt for era constraints
- [ ] Implement anachronism detection
- [ ] Add realism constraint generation
- [ ] Build technology/policing/social norm generation
- [ ] Logging and error handling
- [ ] Unit tests

**Day 8-9: Agent 2 - Cast & Motive**
- [ ] Design prompt for character generation
- [ ] Implement secret/motive/alibi generation
- [ ] Add relationship web generation
- [ ] Integrate with user-provided names
- [ ] Stereotype detection and prevention
- [ ] Logging and tests

**Day 10: Agent 4 - CML Revision**
- [ ] Design prompt for targeted CML fixes
- [ ] Implement validation error parsing
- [ ] Create constraint-preserving revision logic
- [ ] Add novelty divergence corrections
- [ ] Retry strategy (max 3 attempts)
- [ ] Logging and tests

---

### Week 3: Downstream Agents (Clues, Outline, Prose)
**Goal**: Implement post-CML content generation

**Day 11-12: Agent 5 - Clue & Red Herrings**
- [ ] Design prompt for clue extraction from CML
- [ ] Implement "no new facts" validation
- [ ] Build red herring generation tied to false assumption
- [ ] Add clue density logic
- [ ] Fair-play validation checks
- [ ] Logging and tests

**Day 13-14: Agent 6 - Narrative Outliner**
- [ ] Design prompt for chapter structure
- [ ] Implement clue placement logic
- [ ] Add discriminating test timing
- [ ] Build act structure (setup/investigation/resolution)
- [ ] Pacing and tension curve generation
- [ ] Logging and tests

**Day 15: Agent 7 - Prose Writer**
- [ ] Design prompt for narrative prose
- [ ] Implement style matching
- [ ] Add character voice consistency
- [ ] Build clue integration into prose
- [ ] Copyright safety checks
- [ ] Logging and tests

---

### Week 4: Game Pack, Optimization & Production Readiness
**Goal**: Complete all agents, optimize, and prepare for production

**Day 16-17: Agent 8 - Game Pack**
- [ ] Design prompt for interactive materials
- [ ] Implement suspect card generation
- [ ] Build host packet generation
- [ ] Add timeline sheet creation
- [ ] Create printable formatting
- [ ] Logging and tests

**Day 18-19: Optimization & Monitoring**
- [ ] Model routing (reasoning vs fast models)
- [ ] Caching strategies for partial results
- [ ] Cost optimization (token usage tracking)
- [ ] Performance profiling
- [ ] Monitoring dashboard setup
- [ ] Alert system for failures

**Day 20-21: Integration & End-to-End Testing**
- [ ] Full pipeline integration tests
- [ ] Load testing (concurrent requests)
- [ ] Error recovery testing
- [ ] Novelty audit integration
- [ ] User acceptance criteria validation
- [ ] Documentation updates

---

## Logging Infrastructure

### 1. Log Levels
```typescript
enum LogLevel {
  DEBUG = "debug",     // Detailed diagnostic info
  INFO = "info",       // General informational messages
  WARN = "warn",       // Warning messages
  ERROR = "error",     // Error messages
  CRITICAL = "critical" // Critical failures
}
```

### 2. Log Structure
```typescript
interface LLMLogEntry {
  // Request tracking
  timestamp: string;
  runId: string;
  projectId: string;
  agent: string;
  operation: string;
  
  // LLM details
  model: string;
  promptVersion: string;
  temperature: number;
  maxTokens: number;
  
  // Request/response
  promptHash: string;
  responseHash: string;
  prompt?: string; // Full prompt for debugging
  response?: string; // Full response for debugging
  
  // Performance
  latencyMs: number;
  promptTokens: number;
  completionTokens: number;
  totalTokens: number;
  estimatedCost: number;
  
  // Outcome
  success: boolean;
  errorCode?: string;
  errorMessage?: string;
  retryAttempt: number;
  validationStatus?: "pass" | "fail";
  
  // Context
  metadata: Record<string, unknown>;
}
```

### 3. Log Storage
- **Development**: Console + local file (logs/llm-{date}.jsonl)
- **Production**: Database table `llm_logs` + Azure Application Insights
- **Retention**: 90 days for analysis, 1 year for compliance

### 4. Log Queries
```sql
-- Cost analysis by agent
SELECT agent, SUM(estimatedCost) as totalCost, AVG(latencyMs) as avgLatency
FROM llm_logs
WHERE timestamp > NOW() - INTERVAL '7 days'
GROUP BY agent;

-- Failure rate tracking
SELECT agent, operation,
  COUNT(*) as total,
  SUM(CASE WHEN success THEN 1 ELSE 0 END) as successes,
  (1.0 - SUM(CASE WHEN success THEN 1 ELSE 0 END)::float / COUNT(*)) * 100 as failureRate
FROM llm_logs
WHERE timestamp > NOW() - INTERVAL '24 hours'
GROUP BY agent, operation;

-- Token usage trends
SELECT DATE(timestamp) as date,
  SUM(totalTokens) as tokens,
  SUM(estimatedCost) as cost
FROM llm_logs
GROUP BY DATE(timestamp)
ORDER BY date DESC;
```

### 5. Logging Implementation
```typescript
// packages/llm-client/src/logger.ts
export class LLMLogger {
  async logRequest(entry: LLMLogEntry): Promise<void>;
  async logResponse(entry: LLMLogEntry): Promise<void>;
  async logError(entry: LLMLogEntry): Promise<void>;
  async flush(): Promise<void>;
}

// Usage in agent
const logEntry = {
  timestamp: new Date().toISOString(),
  runId,
  projectId,
  agent: "Agent3-CMLGenerator",
  operation: "generate_cml",
  model: "gpt-4",
  // ... rest of fields
};
await logger.logRequest(logEntry);
```

---

## Package Structure

### New Packages

#### 1. `packages/llm-client/`
```
packages/llm-client/
├── package.json
├── tsconfig.json
├── src/
│   ├── index.ts           # Main exports
│   ├── client.ts          # Azure OpenAI client wrapper
│   ├── logger.ts          # Logging infrastructure
│   ├── retry.ts           # Retry logic with backoff
│   ├── ratelimit.ts       # Rate limiting
│   ├── cost-tracker.ts    # Token cost calculation
│   └── types.ts           # Shared types
└── __tests__/
    └── client.test.ts
```

**Key Exports**:
```typescript
export class AzureOpenAIClient {
  async chat(options: ChatOptions): Promise<ChatResponse>;
  async chatWithRetry(options: ChatOptions): Promise<ChatResponse>;
}

export interface ChatOptions {
  messages: Message[];
  model?: string;
  temperature?: number;
  maxTokens?: number;
  jsonMode?: boolean;
  logContext?: LogContext;
}

export interface ChatResponse {
  content: string;
  usage: TokenUsage;
  model: string;
  finishReason: string;
}
```

#### 2. `packages/prompts-llm/`
```
packages/prompts-llm/
├── package.json
├── tsconfig.json
├── src/
│   ├── index.ts
│   ├── agent1-setting.ts      # Setting generation prompts
│   ├── agent2-cast.ts         # Cast generation prompts
│   ├── agent3-cml.ts          # CML generation prompts (CORE)
│   ├── agent4-revision.ts     # CML revision prompts
│   ├── agent5-clues.ts        # Clue generation prompts
│   ├── agent6-outline.ts      # Outline generation prompts
│   ├── agent7-prose.ts        # Prose generation prompts
│   ├── agent8-gamepack.ts     # Game pack generation prompts
│   └── shared/
│       ├── system.ts          # Shared system prompts
│       ├── constraints.ts     # Constraint templates
│       └── schemas.ts         # JSON schema definitions
└── __tests__/
    └── prompts.test.ts
```

**Key Exports**:
```typescript
export function buildCMLPrompt(inputs: CMLPromptInputs): PromptMessages;
export function buildCluesPrompt(inputs: CluesPromptInputs): PromptMessages;
// ... other prompt builders

export interface PromptMessages {
  system: string;
  developer?: string;
  user: string;
  jsonSchema?: object;
}
```

---

## Agent Implementation Details

### Agent 3: CML Generator (CORE)

**Priority**: CRITICAL - This is the backbone of the system

**Prompt Structure**:
```
SYSTEM:
You are an expert mystery writer and logic designer. You create fair-play mystery cases
using the CML 2.0 format. Your mysteries must be:
- Logically sound with no contradictions
- Fair-play compliant (reader has all clues to solve)
- Novel and not derivative of seed examples
- Grounded in the specified era and setting

DEVELOPER:
CML 2.0 Schema: {schema}
Seed CML structural patterns (for inspiration only): {seed_patterns}
Novelty constraints: Must diverge from {similar_seeds} in {divergence_areas}

USER:
Create a mystery case with these specifications:
- Era: {decade}
- Setting: {location} ({institution})
- Primary Axis: {axis}
- Cast: {cast_names}
- Tone: {tone}
- Weather: {weather}
- Social structure: {social_structure}

Requirements:
1. Generate complete CML 2.0 YAML
2. Use provided cast names
3. Build mystery around {axis} axis
4. Ensure false assumption type matches axis
5. Create discriminating test that reveals the truth
6. All clues must be grounded in mechanism/constraint space
7. Fair-play checklist must be satisfied

Output valid CML 2.0 YAML only.
```

**Implementation**:
```typescript
// packages/prompts-llm/src/agent3-cml.ts
export async function generateCML(
  client: AzureOpenAIClient,
  inputs: CMLGenerationInputs,
  logger: LLMLogger
): Promise<CMLGenerationResult> {
  const logContext = {
    runId: inputs.runId,
    projectId: inputs.projectId,
    agent: "Agent3-CMLGenerator",
  };

  // Load seed patterns for structural inspiration
  const seedPatterns = await loadSeedPatterns(inputs.primaryAxis);
  
  // Build prompt
  const prompt = buildCMLPrompt({
    ...inputs,
    seedPatterns,
    noveltyConstraints: inputs.noveltyConstraints,
  });

  let attempt = 0;
  const maxAttempts = 3;

  while (attempt < maxAttempts) {
    attempt++;
    
    try {
      // Generate CML
      const response = await client.chatWithRetry({
        messages: prompt,
        model: "gpt-4", // Use reasoning model for complex logic
        temperature: 0.7,
        maxTokens: 8000,
        jsonMode: false, // YAML output
        logContext: { ...logContext, retryAttempt: attempt },
      });

      // Parse YAML
      const cml = parseYAML(response.content);

      // Validate against schema
      const validation = await validateCml(cml);
      
      if (validation.valid) {
        await logger.logResponse({
          ...logContext,
          operation: "generate_cml",
          success: true,
          validationStatus: "pass",
          retryAttempt: attempt,
          promptHash: hashPrompt(prompt),
          responseHash: hashResponse(response.content),
          latencyMs: response.latencyMs,
          totalTokens: response.usage.totalTokens,
        });

        return { cml, validation, attempt };
      }

      // Validation failed - log and retry
      await logger.logResponse({
        ...logContext,
        operation: "generate_cml",
        success: false,
        validationStatus: "fail",
        errorMessage: validation.errors.join("; "),
        retryAttempt: attempt,
      });

      // If max attempts, use revision agent
      if (attempt >= maxAttempts) {
        return await reviseCML(client, cml, validation, logger, logContext);
      }

    } catch (error) {
      await logger.logError({
        ...logContext,
        operation: "generate_cml",
        success: false,
        errorMessage: error.message,
        retryAttempt: attempt,
      });

      if (attempt >= maxAttempts) {
        throw error;
      }
    }
  }
}
```

**Testing Strategy**:
```typescript
// Test cases for CML generation
describe("Agent 3: CML Generator", () => {
  it("generates valid CML for temporal axis", async () => {
    const result = await generateCML(client, temporalInputs, logger);
    expect(result.validation.valid).toBe(true);
    expect(result.cml.CASE.false_assumption.type).toBe("temporal");
  });

  it("generates valid CML for spatial axis", async () => {
    const result = await generateCML(client, spatialInputs, logger);
    expect(result.validation.valid).toBe(true);
    expect(result.cml.CASE.false_assumption.type).toBe("spatial");
  });

  it("uses provided cast names", async () => {
    const result = await generateCML(client, { castNames: ["Alice", "Bob"] }, logger);
    const castNames = result.cml.CASE.cast.map(c => c.name);
    expect(castNames).toContain("Alice");
    expect(castNames).toContain("Bob");
  });

  it("retries on validation failure", async () => {
    // Mock first attempt to fail validation
    const result = await generateCML(client, inputs, logger);
    expect(result.attempt).toBeGreaterThan(1);
  });

  it("respects novelty constraints", async () => {
    const inputs = {
      ...baseInputs,
      noveltyConstraints: {
        divergeFrom: ["seed1", "seed2"],
        areas: ["mechanism", "false_assumption"],
      },
    };
    const result = await generateCML(client, inputs, logger);
    // Validate novelty in separate test
  });
});
```

---

### Agent 5: Clue Generator (CML-Derived)

**Priority**: HIGH - Must derive from CML facts only

**Prompt Structure**:
```
SYSTEM:
You are a fair-play mystery clue designer. You extract clues from a CML document
without adding any new facts. Every clue must be grounded in the CML's mechanism,
constraint space, or surface model.

DEVELOPER:
CML Document: {cml}
Clue Density: {density}
Red Herring Budget: {red_herring_count}

USER:
Generate clues that:
1. Point to the culprit: {culprit}
2. Support the false assumption: {false_assumption}
3. Are categorized (PHYSICAL, TESTIMONY, DOCUMENT, etc.)
4. Include reveal chapter numbers
5. Red herrings that misdirect toward false assumption

Critical Rules:
- NO NEW FACTS: Every clue must reference existing CML facts
- Fair-play: Load-bearing clues appear early
- Grounding: Each clue cites specific CML section (mechanism, constraint, surface)

Output JSON:
{
  "status": "ready",
  "density": "{density}",
  "axis": "{axis}",
  "summary": "brief description",
  "items": [
    {
      "id": "clue-1",
      "category": "PHYSICAL",
      "text": "clue text",
      "pointsTo": "culprit or misdirection",
      "redHerring": false,
      "revealChapter": 1,
      "cmlSource": "CASE.hidden_model.mechanism.delivery_path[0]"
    }
  ]
}
```

**Validation**:
```typescript
// Validate clues are grounded in CML
function validateCluesGrounding(clues: Clue[], cml: CML): ValidationResult {
  const errors: string[] = [];

  for (const clue of clues.items) {
    if (!clue.cmlSource) {
      errors.push(`Clue ${clue.id} missing cmlSource reference`);
      continue;
    }

    // Verify cmlSource path exists in CML
    const sourceExists = checkCMLPath(cml, clue.cmlSource);
    if (!sourceExists) {
      errors.push(`Clue ${clue.id} references non-existent CML path: ${clue.cmlSource}`);
    }

    // Verify clue text relates to source content
    const sourceContent = getCMLPath(cml, clue.cmlSource);
    const similarity = calculateSimilarity(clue.text, JSON.stringify(sourceContent));
    if (similarity < 0.3) {
      errors.push(`Clue ${clue.id} text doesn't match CML source content`);
    }
  }

  return { valid: errors.length === 0, errors };
}
```

---

## Error Handling Strategy

### 1. Retry Logic
```typescript
interface RetryConfig {
  maxAttempts: number;
  initialDelayMs: number;
  maxDelayMs: number;
  backoffMultiplier: number;
  retryableErrors: string[];
}

const defaultRetryConfig: RetryConfig = {
  maxAttempts: 3,
  initialDelayMs: 1000,
  maxDelayMs: 10000,
  backoffMultiplier: 2,
  retryableErrors: [
    "rate_limit_exceeded",
    "timeout",
    "service_unavailable",
    "connection_error",
  ],
};

async function withRetry<T>(
  fn: () => Promise<T>,
  config: RetryConfig = defaultRetryConfig
): Promise<T> {
  let lastError: Error;
  let delay = config.initialDelayMs;

  for (let attempt = 1; attempt <= config.maxAttempts; attempt++) {
    try {
      return await fn();
    } catch (error) {
      lastError = error;

      // Check if error is retryable
      if (!isRetryableError(error, config.retryableErrors)) {
        throw error;
      }

      // Last attempt - throw
      if (attempt >= config.maxAttempts) {
        throw new Error(`Max retry attempts (${config.maxAttempts}) exceeded: ${error.message}`);
      }

      // Wait before retry with exponential backoff
      await sleep(Math.min(delay, config.maxDelayMs));
      delay *= config.backoffMultiplier;
    }
  }

  throw lastError!;
}
```

### 2. Circuit Breaker
```typescript
class CircuitBreaker {
  private failureCount = 0;
  private lastFailureTime: number = 0;
  private state: "closed" | "open" | "half-open" = "closed";

  constructor(
    private failureThreshold: number = 5,
    private resetTimeoutMs: number = 60000
  ) {}

  async execute<T>(fn: () => Promise<T>): Promise<T> {
    if (this.state === "open") {
      if (Date.now() - this.lastFailureTime > this.resetTimeoutMs) {
        this.state = "half-open";
      } else {
        throw new Error("Circuit breaker is OPEN");
      }
    }

    try {
      const result = await fn();
      this.onSuccess();
      return result;
    } catch (error) {
      this.onFailure();
      throw error;
    }
  }

  private onSuccess(): void {
    this.failureCount = 0;
    this.state = "closed";
  }

  private onFailure(): void {
    this.failureCount++;
    this.lastFailureTime = Date.now();

    if (this.failureCount >= this.failureThreshold) {
      this.state = "open";
    }
  }
}
```

### 3. Fallback Strategies
```typescript
async function generateCMLWithFallback(inputs: CMLInputs): Promise<CML> {
  try {
    // Primary: Try with GPT-4 (reasoning model)
    return await generateCML(inputs, { model: "gpt-4" });
  } catch (error) {
    logger.warn("GPT-4 generation failed, trying GPT-3.5-turbo", { error });

    try {
      // Fallback 1: Try with GPT-3.5-turbo (faster, cheaper)
      return await generateCML(inputs, { model: "gpt-3.5-turbo" });
    } catch (error2) {
      logger.error("All LLM generation attempts failed", { error: error2 });

      // Fallback 2: Return deterministic CML (Phase 1 behavior)
      logger.warn("Using deterministic fallback CML");
      return deriveCML(inputs.spec);
    }
  }
}
```

---

## Cost Optimization

### 1. Token Budgets
```typescript
const TOKEN_BUDGETS = {
  agent1_setting: { prompt: 500, completion: 1000 },
  agent2_cast: { prompt: 800, completion: 1500 },
  agent3_cml: { prompt: 2000, completion: 6000 }, // Largest
  agent4_revision: { prompt: 3000, completion: 2000 },
  agent5_clues: { prompt: 3000, completion: 1500 },
  agent6_outline: { prompt: 4000, completion: 2000 },
  agent7_prose: { prompt: 5000, completion: 8000 }, // Largest
  agent8_gamepack: { prompt: 4000, completion: 3000 },
};

function estimateCost(agent: string, tokens: TokenUsage): number {
  // GPT-4 pricing (example, update with actual pricing)
  const COST_PER_1K_PROMPT = 0.03;
  const COST_PER_1K_COMPLETION = 0.06;

  return (
    (tokens.promptTokens / 1000) * COST_PER_1K_PROMPT +
    (tokens.completionTokens / 1000) * COST_PER_1K_COMPLETION
  );
}
```

### 2. Caching Strategy
```typescript
interface CacheEntry {
  key: string;
  value: string;
  timestamp: number;
  ttl: number;
}

class LLMCache {
  private cache = new Map<string, CacheEntry>();

  async get(key: string): Promise<string | null> {
    const entry = this.cache.get(key);
    if (!entry) return null;

    // Check TTL
    if (Date.now() - entry.timestamp > entry.ttl) {
      this.cache.delete(key);
      return null;
    }

    return entry.value;
  }

  async set(key: string, value: string, ttl: number = 3600000): Promise<void> {
    this.cache.set(key, {
      key,
      value,
      timestamp: Date.now(),
      ttl,
    });
  }

  // Generate cache key from inputs
  static cacheKey(agent: string, inputs: Record<string, unknown>): string {
    return `${agent}:${hashObject(inputs)}`;
  }
}

// Usage
async function generateWithCache(agent: string, inputs: any): Promise<any> {
  const cacheKey = LLMCache.cacheKey(agent, inputs);
  const cached = await cache.get(cacheKey);

  if (cached) {
    logger.info("Cache hit", { agent, cacheKey });
    return JSON.parse(cached);
  }

  const result = await generateLLM(agent, inputs);
  await cache.set(cacheKey, JSON.stringify(result));
  return result;
}
```

### 3. Model Routing
```typescript
const MODEL_ROUTING = {
  // Complex reasoning tasks - use GPT-4
  agent3_cml: "gpt-4",
  agent4_revision: "gpt-4",
  agent5_clues: "gpt-4",

  // Simpler tasks - use GPT-3.5-turbo
  agent1_setting: "gpt-3.5-turbo",
  agent2_cast: "gpt-3.5-turbo",
  agent6_outline: "gpt-3.5-turbo",

  // Creative writing - use GPT-4 for quality
  agent7_prose: "gpt-4",
  agent8_gamepack: "gpt-3.5-turbo",
};

function selectModel(agent: string): string {
  return MODEL_ROUTING[agent] || "gpt-3.5-turbo";
}
```

---

## Monitoring & Alerting

### 1. Metrics to Track
```typescript
interface LLMMetrics {
  // Throughput
  requestsPerMinute: number;
  successRate: number;
  
  // Performance
  avgLatencyMs: number;
  p95LatencyMs: number;
  p99LatencyMs: number;
  
  // Cost
  totalTokensUsed: number;
  estimatedCostUSD: number;
  costPerRequest: number;
  
  // Quality
  validationFailureRate: number;
  retryRate: number;
  fallbackRate: number;
  
  // By agent
  metricsByAgent: Map<string, AgentMetrics>;
}
```

### 2. Alert Conditions
```typescript
const ALERTS = [
  {
    name: "High Failure Rate",
    condition: (metrics) => metrics.successRate < 0.9,
    severity: "critical",
    message: "LLM success rate below 90%",
  },
  {
    name: "High Latency",
    condition: (metrics) => metrics.p95LatencyMs > 10000,
    severity: "warning",
    message: "P95 latency exceeds 10 seconds",
  },
  {
    name: "Cost Spike",
    condition: (metrics) => metrics.costPerRequest > 0.50,
    severity: "warning",
    message: "Cost per request exceeds $0.50",
  },
  {
    name: "Circuit Breaker Open",
    condition: (metrics) => metrics.circuitBreakerStatus === "open",
    severity: "critical",
    message: "Circuit breaker is open - service degraded",
  },
];
```

### 3. Dashboard Queries
```typescript
// Real-time metrics dashboard
const dashboardQueries = {
  successRate: `
    SELECT 
      COUNT(*) FILTER (WHERE success = true)::float / COUNT(*) * 100 as success_rate
    FROM llm_logs
    WHERE timestamp > NOW() - INTERVAL '5 minutes'
  `,
  
  latencyP95: `
    SELECT PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY latencyMs) as p95_latency
    FROM llm_logs
    WHERE timestamp > NOW() - INTERVAL '5 minutes'
  `,
  
  costByAgent: `
    SELECT agent, SUM(estimatedCost) as total_cost
    FROM llm_logs
    WHERE timestamp > NOW() - INTERVAL '1 hour'
    GROUP BY agent
    ORDER BY total_cost DESC
  `,
  
  errorsByType: `
    SELECT errorCode, COUNT(*) as count
    FROM llm_logs
    WHERE success = false AND timestamp > NOW() - INTERVAL '1 hour'
    GROUP BY errorCode
    ORDER BY count DESC
  `,
};
```

---

## Testing Strategy

### 1. Unit Tests
```typescript
// Test each agent in isolation
describe("Agent 3: CML Generator", () => {
  let client: MockLLMClient;
  let logger: MockLogger;

  beforeEach(() => {
    client = new MockLLMClient();
    logger = new MockLogger();
  });

  it("generates valid CML", async () => {
    const result = await generateCML(client, inputs, logger);
    expect(result.validation.valid).toBe(true);
  });

  it("handles validation failures", async () => {
    client.mockValidationFailure();
    const result = await generateCML(client, inputs, logger);
    expect(result.attempt).toBeGreaterThan(1);
  });

  it("logs all operations", async () => {
    await generateCML(client, inputs, logger);
    expect(logger.entries.length).toBeGreaterThan(0);
  });
});
```

### 2. Integration Tests
```typescript
// Test full pipeline
describe("Full LLM Pipeline", () => {
  it("generates complete mystery from spec", async () => {
    const spec = createTestSpec();
    const result = await runFullPipeline(spec);
    
    expect(result.setting).toBeDefined();
    expect(result.cast).toBeDefined();
    expect(result.cml).toBeDefined();
    expect(result.clues).toBeDefined();
    expect(result.outline).toBeDefined();
  });

  it("derives all content from CML", async () => {
    const result = await runFullPipeline(spec);
    
    // Verify clues reference CML facts
    for (const clue of result.clues.items) {
      expect(clue.cmlSource).toBeDefined();
      const sourceExists = checkCMLPath(result.cml, clue.cmlSource);
      expect(sourceExists).toBe(true);
    }
    
    // Verify outline uses CML characters
    const cmlCast = result.cml.CASE.cast.map(c => c.name);
    const outlineHasAllCharacters = cmlCast.every(name =>
      result.outline.chapters.some(ch => ch.summary.includes(name))
    );
    expect(outlineHasAllCharacters).toBe(true);
  });
});
```

### 3. Load Tests
```typescript
// Test concurrent requests
describe("Load Testing", () => {
  it("handles 10 concurrent requests", async () => {
    const requests = Array.from({ length: 10 }, (_, i) =>
      runFullPipeline(createTestSpec({ projectId: `proj-${i}` }))
    );
    
    const results = await Promise.all(requests);
    
    expect(results.every(r => r.cml)).toBe(true);
    expect(results.every(r => r.validation.valid)).toBe(true);
  });

  it("stays within rate limits", async () => {
    // Should not exceed 60 requests per minute
    const start = Date.now();
    const requests = Array.from({ length: 70 }, () => generateCML(client, inputs));
    
    await expect(Promise.all(requests)).rejects.toThrow("rate_limit_exceeded");
    const elapsed = Date.now() - start;
    expect(elapsed).toBeGreaterThan(60000);
  });
});
```

### 4. Quality Tests
```typescript
// Test output quality
describe("Output Quality", () => {
  it("CML passes validation", async () => {
    const result = await generateCML(client, inputs, logger);
    const validation = await validateCml(result.cml);
    expect(validation.valid).toBe(true);
    expect(validation.errors).toHaveLength(0);
  });

  it("CML is novel compared to seeds", async () => {
    const result = await generateCML(client, inputs, logger);
    const novelty = await checkNovelty(result.cml, seedCMLs);
    expect(novelty.score).toBeGreaterThan(0.7); // 70% different
  });

  it("clues are grounded in CML", async () => {
    const result = await runFullPipeline(spec);
    const grounding = validateCluesGrounding(result.clues, result.cml);
    expect(grounding.valid).toBe(true);
  });
});
```

---

## Environment Configuration

### .env.local
```bash
# Azure OpenAI
AZURE_OPENAI_API_KEY=your_api_key_here
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_API_VERSION=2023-12-01-preview
AZURE_OPENAI_DEPLOYMENT_GPT4=gpt-4
AZURE_OPENAI_DEPLOYMENT_GPT35=gpt-35-turbo

# Logging
LOG_LEVEL=info
LOG_TO_FILE=true
LOG_FILE_PATH=./logs/llm.jsonl
LOG_TO_CONSOLE=true

# Performance
LLM_TIMEOUT_MS=30000
LLM_MAX_RETRIES=3
LLM_RATE_LIMIT_PER_MINUTE=60

# Circuit Breaker
CIRCUIT_BREAKER_FAILURE_THRESHOLD=5
CIRCUIT_BREAKER_RESET_TIMEOUT_MS=60000

# Caching
ENABLE_LLM_CACHE=true
CACHE_TTL_MS=3600000

# Cost Limits
MAX_COST_PER_REQUEST_USD=1.00
ALERT_COST_THRESHOLD_USD=0.50

# Monitoring
ENABLE_METRICS=true
METRICS_INTERVAL_MS=60000
ENABLE_ALERTS=true
```

---

## Migration Strategy

### Phase 2A: Infrastructure (Week 1)
**Deliverables**:
- [ ] Azure OpenAI client package
- [ ] Logging infrastructure
- [ ] Agent 3 (CML Generator) implementation
- [ ] Basic monitoring dashboard

**Success Criteria**:
- CML generated by LLM validates successfully
- All logs captured and queryable
- Performance within acceptable range (<10s per CML)

### Phase 2B: Upstream Agents (Week 2)
**Deliverables**:
- [ ] Agent 1 (Setting)
- [ ] Agent 2 (Cast)
- [ ] Agent 4 (CML Revision)
- [ ] Integration with Agent 3

**Success Criteria**:
- Setting and cast feed into CML generation
- CML auto-revision works on validation failure
- End-to-end pipeline from spec to validated CML

### Phase 2C: Downstream Agents (Week 3)
**Deliverables**:
- [ ] Agent 5 (Clues)
- [ ] Agent 6 (Outline)
- [ ] Agent 7 (Prose)
- [ ] Grounding validation

**Success Criteria**:
- All downstream artifacts derive from CML
- No new facts added outside CML
- Clues reference specific CML sources

### Phase 2D: Production Readiness (Week 4)
**Deliverables**:
- [ ] Agent 8 (Game Pack)
- [ ] Full test suite
- [ ] Monitoring dashboard
- [ ] Alert system
- [ ] Documentation

**Success Criteria**:
- 95%+ success rate
- <10s average latency
- Cost per mystery <$2
- All alerts configured

---

## Rollout Plan

### Stage 1: Internal Testing (Days 1-3 after completion)
- Deploy to development environment
- Run 100 test generations
- Monitor for errors and performance issues
- Adjust prompts and parameters

### Stage 2: Beta Testing (Days 4-7)
- Enable for 10% of users (feature flag)
- Monitor quality, performance, cost
- Collect user feedback
- Compare to Phase 1 deterministic output

### Stage 3: Gradual Rollout (Days 8-14)
- Increase to 25% of users
- Increase to 50% of users
- Increase to 75% of users
- Monitor at each stage

### Stage 4: Full Production (Day 15+)
- Enable for 100% of users
- Remove Phase 1 deterministic fallbacks (keep as emergency fallback)
- Continuous monitoring and optimization

---

## Risk Mitigation

### Risk 1: High Latency
**Mitigation**:
- Implement caching for repeated inputs
- Use faster models for non-critical agents
- Parallel execution where possible
- Set timeout limits and fail gracefully

### Risk 2: High Cost
**Mitigation**:
- Set per-request cost limits
- Use model routing (GPT-3.5 for simpler tasks)
- Implement caching to reduce redundant calls
- Monitor cost trends and alert on spikes

### Risk 3: Quality Issues
**Mitigation**:
- Comprehensive validation at each stage
- Auto-retry on validation failure
- Fallback to deterministic generation if LLM fails
- Continuous quality monitoring

### Risk 4: Rate Limiting
**Mitigation**:
- Request queue with rate limiting
- Backoff and retry logic
- Multiple API keys for higher throughput
- Circuit breaker to prevent cascade failures

### Risk 5: Service Outage
**Mitigation**:
- Circuit breaker pattern
- Fallback to Phase 1 deterministic generation
- Health check endpoints
- Automatic alerting and failover

---

## Success Metrics

### Quality
- [ ] CML validation pass rate >95%
- [ ] Novelty score >70% vs seeds
- [ ] Fair-play compliance 100%
- [ ] Clue grounding validation 100%

### Performance
- [ ] Average latency <8s per agent
- [ ] P95 latency <15s per agent
- [ ] Success rate >95%
- [ ] Retry rate <10%

### Cost
- [ ] Cost per mystery <$2
- [ ] Token usage within budgets
- [ ] Cache hit rate >30%

### Reliability
- [ ] Uptime >99.5%
- [ ] Circuit breaker trips <5 per day
- [ ] Fallback activation <1%

---

## Documentation Requirements

### Developer Documentation
- [ ] API reference for all agents
- [ ] Prompt engineering guide
- [ ] Logging and monitoring guide
- [ ] Troubleshooting guide
- [ ] Cost optimization tips

### User Documentation
- [ ] Update LLM_GENERATION_MAP.md with implementation status
- [ ] Update workflow documentation
- [ ] Add FAQ about LLM behavior
- [ ] Explain generation times and cost

### Operations Documentation
- [ ] Deployment guide
- [ ] Monitoring setup
- [ ] Alert response procedures
- [ ] Cost management procedures
- [ ] Incident response playbook

---

## Appendix: Prompt Templates

### Agent 3: CML Generator Prompt (Full)
```
SYSTEM:
You are an expert mystery designer creating fair-play whodunit mysteries in CML 2.0 format.
Your expertise includes: Golden Age detective fiction, logical puzzle construction, fair-play
mystery conventions, and narrative misdirection techniques.

Core Principles:
1. Fair Play: The reader must have access to all clues needed to solve the mystery
2. Logical Consistency: No contradictions in timeline, access, or physical constraints
3. Novelty: Each mystery must be unique and not derivative of existing works
4. Solvability: A careful reader should be able to deduce the solution before the reveal

CML Structure Requirements:
- Complete CML 2.0 YAML document
- All required sections: meta, cast, culpability, surface_model, hidden_model, false_assumption,
  constraint_space, inference_path, discriminating_test, fair_play
- Valid enum values for axis types and discriminating test methods
- Internally consistent facts across all sections

DEVELOPER:
CML 2.0 Schema:
{full_schema_here}

Seed Pattern Analysis (structural inspiration only - DO NOT COPY CONTENT):
{seed_structural_patterns}

Novelty Constraints:
- Must diverge from seed cases: {similar_seed_ids}
- Required divergence in: {divergence_areas}
- Specific avoidances: {avoidance_patterns}

USER:
Create a complete mystery case with these specifications:

Setting & Era:
- Decade: {decade}
- Location: {location_preset} ({institution})
- Weather: {weather}
- Social Structure: {social_structure}
- Tone: {tone}

Cast Requirements:
- Size: {cast_size} characters
- Names: {cast_names} (use these exact names)
- Detective Type: {detective_type}
- Victim Archetype: {victim_archetype}

Mystery Logic:
- Primary Axis: {primary_axis}
- False Assumption Type: {false_assumption_type} (must match axis)
- Mechanism Families: {mechanism_families}
- Complexity: {complexity_level}

Requirements:
1. Build the entire mystery around the {primary_axis} axis
2. The false assumption must be of type {false_assumption_type}
3. Use all provided character names in the cast section
4. Create a mechanism that exploits the false assumption
5. Design constraint spaces that hide the truth but allow fair deduction
6. Build an inference path with 3-5 steps that correct the false assumption
7. Create a discriminating test that definitively proves guilt
8. Ensure all clues are grounded in mechanism or constraint violations
9. The fair_play checklist must all be true

Output:
Generate complete, valid CML 2.0 YAML. Start with "CML_VERSION: 2.0" and include
all required sections. Do not include explanations or commentary outside the YAML.
```

---

## Next Steps

1. **Week 1 Kickoff**: Set up Azure OpenAI, create packages, implement Agent 3
2. **Daily Standups**: Review progress, address blockers
3. **Weekly Review**: Assess quality, cost, performance metrics
4. **Post-Implementation**: Conduct retrospective, document lessons learned

**Questions/Concerns**: Raise to tech lead immediately for resolution

**Success Definition**: Phase 2 complete when all 8 agents are production-ready, fully logged, and generating high-quality mysteries at <$2 per generation.
